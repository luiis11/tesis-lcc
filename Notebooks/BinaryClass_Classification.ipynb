{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRiyt//YTBAGPcgforYUGy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiis11/tesis-lcc/blob/main/Notebooks/BinaryClass_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Library Imports"
      ],
      "metadata": {
        "id": "nkKo_NgVByw-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeBSz8JP7RnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d151fd8-c01a-4a62-8675-44499752f19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mglearn\n",
            "  Downloading mglearn-0.1.9.tar.gz (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mglearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mglearn) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from mglearn) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from mglearn) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from mglearn) (8.4.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.9/dist-packages (from mglearn) (0.11.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (from mglearn) (2.25.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from mglearn) (1.2.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mglearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->mglearn) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->mglearn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->mglearn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mglearn) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mglearn) (1.16.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=9d88b2f6455efff5586eabc6b23d8f69e0be6422bedaa3de3ac381f7abddc4d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/30/8a/1e2c8f144c9d411b0688f628d496d2db8f61b6d14bcedaa1df\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# General Libraries\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os               # para manipulacion de directorios\n",
        "import shutil           # para eliminar carpetas no vacias\n",
        "!pip install mglearn    # el comando 'import mglearn' no funcionaba\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# CountVectorizer | TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Cross Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# GridSearch\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# LogisticRegression Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# SVM Classifier\n",
        "from sklearn import svm\n",
        "\n",
        "# KNN Clasificator\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "\n",
        "# Gaussian NaiveBayes Clasificator\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# DecisionTree (Classifier and Regressor)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Graphics library\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read CSV\n",
        "**Selección del la personalidad**\n",
        "\n",
        "Se utiliza un valor dentro del rango de las personalidades para entrenar el clasificador binario en la personalidad seleccionada.\n",
        "Los cuales tienen un total de 225 filas y cuentan con la siguiente estructura en columnas:\n",
        "\n",
        "*   ID: Number\n",
        "*   Personality: ['si', 'no']\n",
        "*   Autodescripción: String"
      ],
      "metadata": {
        "id": "uRLTtG05Jft2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the Personality\n",
        "\n",
        "# 0: Agrado\n",
        "# 1: Apertura\n",
        "# 2: Extraversion\n",
        "# 3: Neuroticismo\n",
        "# 4: Responsabilidad\n",
        "\n",
        "POS = 4\n",
        "\n",
        "personality = [\"Agrado\", \"Apertura\", \"Extraversion\", \"Neuroticismo\", \"Responsabilidad\"]\n",
        "colum_title = [\"AgradPerc\", \"ApertPerc\", \"ExtravPerc\", \"NeurPerc\", \"RespPerc\"]\n",
        "\n",
        "path_colab_original = [\n",
        "    \"https://drive.google.com/file/d/138orceXr83_CdJiVvC7bb9IF4-J8P6fp/view\", #\"./Agrado_Full.csv\",\n",
        "    \"https://drive.google.com/file/d/1598rz27p2WRDtq6W1kQF1Pp0uigzpB_U/view\", #\"./Apertura_Full.csv\" ,\n",
        "    \"https://drive.google.com/file/d/187RaPP8EnCjetblpr6YuHuCIXuASVDoa/view\", #\"./Extraversion_Full.csv\" ,\n",
        "    \"https://drive.google.com/file/d/18h2J-jBaPXbhROjLnglukvYaF_Mv4BmN/view\", #\"./Neuroticismo_Full.csv\" ,\n",
        "    \"https://drive.google.com/file/d/1wr20m6Cr7HL91rnbM3t5tfDr5GTZcHpS/view\", #\"./#Responsabilidad_Full.csv\"\n",
        "    #\"https://drive.google.com/file/d/17ETdIV9lL2TRBrCp3ZHq7g2emAxOSqZT/view\", #\"./Responsabilidad_Full.csv\"\n",
        "]\n",
        "\n",
        "path_colab_augment  = [\n",
        "    \"https://drive.google.com/file/d/1MvPCzl_1C7LRvo9Yev10e-46qWiI-D08/view\", #\"./Augment_gpt_Agrado.csv\",\n",
        "    \"https://drive.google.com/file/d/1VSQl39Q1pU9zmpkAjKbi2ZhB96_qpe1r/view\", #\"./Augment_gpt_Apertura.csv\",\n",
        "    \"https://drive.google.com/file/d/19vFX82WeQuB2_TXTw4VUUrl3iqYM54E2/view\", #\"./Augment_gpt_Extraversion.csv\",\n",
        "    \"https://drive.google.com/file/d/1JB-g343XOFtDNIiuK1jV1aOfzcLNY_w1/view\", #\"./Augment_gpt_Neuroticismo.csv\",\n",
        "    \"https://drive.google.com/file/d/1WeA1BUlsGWYvC6HUm5lO01XQ8d09ssJZ/view\", #\"./Augmented_gpt3_RespPerc.csv\"    (max: 0.88)\n",
        "    #\"https://drive.google.com/file/d/1ctiemALPgO5rCpDe76BgSRMwU_1LlwLh/view\", #\"./Augmented_gpt2_RespPerc.csv\" (max: 0.84)\n",
        "    #\"https://drive.google.com/file/d/1btMU7xKmrZN16ND34bcHQ-V31XGjBE9m/view\", #\"./Augmented_gpt3_RespPerc_FIX.csv\" (max: 0.83)\n",
        "    #\"https://drive.google.com/file/d/1oEc2U6IGUvYsuZyBOrfjP8pHxbNmRZA-/view\", #\"./Augmented_gpt3_RespPerc_FIX2.csv\"  (max: 0.81)\n",
        "    #\"https://drive.google.com/file/d/1-U1N66k4KfBClL2bM4atEpVby643keVm/view\", #\"./Augmented_RespPerc.csv\" (best results of augment without gpt)\n",
        "    #\"https://drive.google.com/file/d/1eF5hUEEsMxlVAPWpzHQnEaXaWNZzxvnR/view\", #\"./Augmented_gpt3_RespPerc.csv\"\n",
        "    #\"https://drive.google.com/file/d/1lt7U63P-tSD7fPgshEFMjw651qCGihys/view\", #\"./Augment_All_Classes_number.csv\"\n",
        "    #\"https://drive.google.com/file/d/1FFyWdFr-qpY2AsjRG8XoMo-JkcyxmsLg/view\", #\"./Augment_v2_All_Classes_number.csv\" (To Test Agrado)\n",
        "]\n",
        "\n",
        "path_colab = path_colab_augment # Select the augment dataset\n",
        "\n",
        "\n",
        "# Set values\n",
        "path_colab = path_colab[POS]\n",
        "colum_labels = colum_title[POS]\n",
        "\n",
        "# With ONLY URL\n",
        "if path_colab.startswith('http'):\n",
        "  print(\"Ubication WEB\")\n",
        "  path_colab='https://drive.google.com/uc?id=' + path_colab.split('/')[-2]\n",
        "else: print(\"Ubication Local\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqV0cvkhCyEb",
        "outputId": "e2ad856c-c26b-4872-a6d2-c488340caacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ubication WEB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar un CSV con pandas a un formato DataFrame (df)\n",
        "\n",
        "df = pd.read_csv(\n",
        "    path_colab,\n",
        "    encoding = \"utf-8\",\n",
        "    header = 0\n",
        ")\n",
        "#####################\n",
        "## Función para contar el número de palabras en un texto\n",
        "#def contar_palabras(texto):\n",
        "#    palabras = re.findall(r'\\b\\w+\\b', texto)\n",
        "#    return len(palabras)\n",
        "\n",
        "## Crea una nueva columna \"num_palabras\" con el número de palabras en cada texto\n",
        "#df['num_palabras'] = df['Autodescripción'].apply(contar_palabras)\n",
        "\n",
        "## Filtra las filas donde el número de palabras es mayor a 80\n",
        "#df_filtrado = df[df['num_palabras'] > 330]\n",
        "\n",
        "#df_filtrado = df_filtrado[df_filtrado['ID'] != 410]\n",
        "#df_filtrado = df_filtrado[df_filtrado['ID'] != 445]\n",
        "\n",
        "## Elimina la columna \"num_palabras\"\n",
        "#df = df_filtrado.drop('num_palabras', axis=1)\n",
        "\n",
        "#####################\n",
        "texts_unprocessed = list(df[\"Autodescripción\"]) # será el X_train o simplemente X\n",
        "labels = list(df[colum_labels])                 # será el y_train o simplemente y\n",
        "print(\"Textos:   \", len(texts_unprocessed))\n",
        "print(\"Etiquetas:\", len(labels))\n",
        "\n",
        "#df = df.drop(df.tail(24).index) # Remover ultimas n filas\n",
        "\n",
        "print(f\"+({df[colum_labels].sum()})\")\n",
        "print(f\"-({len(df)-df[colum_labels].sum()})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohajXUXZJEsh",
        "outputId": "ae91da39-7f3d-460b-a07f-8af08c22f3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos:    470\n",
            "Etiquetas: 470\n",
            "+(284)\n",
            "-(186)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento\n",
        "**Preprocesamiento del texto (opcional)**\n",
        "\n",
        "Es posible realizar una limpieza del texto, quitando carácteres innecesarios como números, símbolos, e incluso palabras que no aportan significancia a la oración.\n",
        "\n",
        "Antes de aplicar la funcion CountVectorizer o TfidfVectorizer (es decir, obtener la representación), es posible realizar un preprocesamiento del texto de entrada para ´limpiarlo´ un poco.\n",
        "\n",
        "En este caso, luego de leer el corpus se procede a preprocesar el texto de la columna Autodescripción con las siguientes técnicas."
      ],
      "metadata": {
        "id": "clNA1nToKxwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texto):\n",
        "    texto = texto.replace('.', '. ').replace(',', ', ').replace(';', '; ').replace('(', '( ').replace(')', ') ') # Separar palabras antes de quitar las comas y puntos para evitar que palabras queden juntas\n",
        "    texto = re.sub(r'[^\\w ]', \"\", texto)      # Eliminar todo lo que no sea una palabra\n",
        "\n",
        "    texto = re.sub(r'\\d+', 'numero', texto) # Cambiar numeros por la palabra numero\n",
        "    texto = texto.lower()                     # Pasa el texto a minúscula\n",
        "    letras_dobles = \"abdfghijkmnñpqstuvwxyz\"  # Remover letras con 2 ocurrencias (con excepciones). Excepciones: ee-cc-ll-rr-oo (y mayus)\n",
        "    letras_dobles += letras_dobles.upper()\n",
        "    texto = re.sub(\"(?P<char>[\" + re.escape(letras_dobles) + \"])(?P=char)+\", r\"\\1\", texto)\n",
        "    texto = re.sub(r'([\\w\\W])\\1{2,}', r'\\1', texto)  # remover caracteres que se repiten al menos 3 veces\n",
        "    return texto\n",
        "\n",
        "texts_processed = []\n",
        "\n",
        "for i in texts_unprocessed:\n",
        "    x = preprocessing(i)\n",
        "    texts_processed.append(x)\n",
        "\n",
        "#print(\"Muestra procesada(size=%d): %s\"       %( contar_palabras(texts_processed[0]), texts_processed[0]) )\n",
        "#print(\"\\nMuestra sin procesar(size=%d): %s\"  %( contar_palabras(texts_unprocessed[0]), texts_unprocessed[0]) )\n",
        "\n",
        "\n",
        "# Selección de Texto original o preprocesado (Comentar el que no se utiliza)\n",
        "\n",
        "# texts = texts_unprocessed\n",
        "texts = texts_processed"
      ],
      "metadata": {
        "id": "WPoT8LtbKzN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizar stop_words extendido\n",
        "\n",
        "# stopwords_sp_web = [\"algún\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"ambos\",\"ampleamos\",\"ante\",\"antes\",\"aquel\",\"aquellas\",\"aquellos\",\"aqui\",\"arriba\",\"atras\",\"bajo\",\"bastante\",\"bien\",\"cada\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"como\",\"con\",\"conseguimos\",\"conseguir\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"cual\",\"cuando\",\"dentro\",\"desde\",\"donde\",\"dos\",\"el\",\"ellas\",\"ellos\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esta\",\"estaba\",\"estado\",\"estais\",\"estamos\",\"estan\",\"estoy\",\"fin\",\"fue\",\"fueron\",\"fui\",\"fuimos\",\"gueno\",\"ha\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"haces\",\"hago\",\"incluso\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"la\",\"largo\",\"las\",\"lo\",\"los\",\"mientras\",\"mio\",\"modo\",\"muchos\",\"muy\",\"nos\",\"nosotros\",\"otro\",\"para\",\"pero\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"por\",\"por qué\",\"porque\",\"primero\",\"puede\",\"pueden\",\"puedo\",\"quien\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"ser\",\"si\",\"siendo\",\"sin\",\"sobre\",\"sois\",\"solamente\",\"solo\",\"somos\",\"soy\",\"su\",\"sus\",\"también\",\"teneis\",\"tenemos\",\"tener\",\"tengo\",\"tiempo\",\"tiene\",\"tienen\",\"todo\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"tuyo\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"vaya\",\"verdad\",\"verdadera\",\"verdadero\",\"vosotras\",\"vosotros\",\"voy\",\"yo\",\"él\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\",\"a\",\"añadió\",\"aún\",\"actualmente\",\"adelante\",\"además\",\"afirmó\",\"agregó\",\"ahí\",\"ahora\",\"al\",\"algo\",\"alrededor\",\"anterior\",\"apenas\",\"aproximadamente\",\"aquí\",\"así\",\"aseguró\",\"aunque\",\"ayer\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"cómo\",\"casi\",\"cerca\",\"cinco\",\"comentó\",\"conocer\",\"consideró\",\"considera\",\"contra\",\"cosas\",\"creo\",\"cuales\",\"cualquier\",\"cuanto\",\"cuatro\",\"cuenta\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"demás\",\"después\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"durante\",\"e\",\"ejemplo\",\"ella\",\"ello\",\"embargo\",\"encuentra\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"está\",\"están\",\"estaban\",\"estar\",\"estará\",\"estas\",\"este\",\"esto\",\"estos\",\"estuvo\",\"ex\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"fuera\",\"gran\",\"grandes\",\"había\",\"habían\",\"haber\",\"habrá\",\"hacerlo\",\"hacia\",\"haciendo\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"hoy\",\"hubo\",\"igual\",\"indicó\",\"informó\",\"junto\",\"lado\",\"le\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"luego\",\"lugar\",\"más\",\"manera\",\"manifestó\",\"mayor\",\"me\",\"mediante\",\"mejor\",\"mencionó\",\"menos\",\"mi\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"nada\",\"nadie\",\"ni\",\"ningún\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"no\",\"nosotras\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"otra\",\"otras\",\"otros\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"posible\",\"próximo\",\"próximos\",\"primer\",\"primera\",\"primeros\",\"principalmente\",\"propia\",\"propias\",\"propio\",\"propios\",\"pudo\",\"pueda\",\"pues\",\"qué\",\"que\",\"quedó\",\"queremos\",\"quién\",\"quienes\",\"quiere\",\"realizó\",\"realizado\",\"realizar\",\"respecto\",\"sí\",\"sólo\",\"se\",\"señaló\",\"sea\",\"sean\",\"según\",\"segunda\",\"segundo\",\"seis\",\"será\",\"serán\",\"sería\",\"sido\",\"siempre\",\"siete\",\"sigue\",\"siguiente\",\"sino\",\"sola\",\"solas\",\"solos\",\"son\",\"tal\",\"tampoco\",\"tan\",\"tanto\",\"tenía\",\"tendrá\",\"tendrán\",\"tenga\",\"tenido\",\"tercera\",\"toda\",\"todas\",\"todavía\",\"todos\",\"total\",\"trata\",\"través\",\"tres\",\"tuvo\",\"usted\",\"varias\",\"varios\",\"veces\",\"ver\",\"vez\",\"y\",\"ya\"]\n",
        "stopwords_sp_web_accent_less = set({\"algun\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"ambos\",\"ampleamos\",\"ante\",\"antes\",\"aquel\",\"aquellas\",\"aquellos\",\"aqui\",\"arriba\",\"atras\",\"bajo\",\"bastante\",\"bien\",\"cada\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"como\",\"con\",\"conseguimos\",\"conseguir\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"cual\",\"cuando\",\"dentro\",\"desde\",\"donde\",\"dos\",\"el\",\"ellas\",\"ellos\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"entonces\",\"entre\",\"era\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esta\",\"estaba\",\"estado\",\"estais\",\"estamos\",\"estan\",\"estoy\",\"fin\",\"fue\",\"fueron\",\"fui\",\"fuimos\",\"gueno\",\"ha\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"haces\",\"hago\",\"incluso\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"la\",\"largo\",\"las\",\"lo\",\"los\",\"mientras\",\"mio\",\"modo\",\"muchos\",\"muy\",\"nos\",\"nosotros\",\"otro\",\"para\",\"pero\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"por\",\"por que\",\"porque\",\"primero\",\"puede\",\"pueden\",\"puedo\",\"quien\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"ser\",\"si\",\"siendo\",\"sin\",\"sobre\",\"sois\",\"solamente\",\"solo\",\"somos\",\"soy\",\"su\",\"sus\",\"tambien\",\"teneis\",\"tenemos\",\"tener\",\"tengo\",\"tiempo\",\"tiene\",\"tienen\",\"todo\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"tuyo\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"vaya\",\"verdad\",\"verdadera\",\"verdadero\",\"vosotras\",\"vosotros\",\"voy\",\"yo\",\"el\",\"esta\",\"estas\",\"este\",\"estos\",\"ultima\",\"ultimas\",\"ultimo\",\"ultimos\",\"a\",\"anadio\",\"aun\",\"actualmente\",\"adelante\",\"ademas\",\"afirmo\",\"agrego\",\"ahi\",\"ahora\",\"al\",\"algo\",\"alrededor\",\"anterior\",\"apenas\",\"aproximadamente\",\"aqui\",\"asi\",\"aseguro\",\"aunque\",\"ayer\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"como\",\"casi\",\"cerca\",\"cinco\",\"comento\",\"conocer\",\"considero\",\"considera\",\"contra\",\"cosas\",\"creo\",\"cuales\",\"cualquier\",\"cuanto\",\"cuatro\",\"cuenta\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejo\",\"del\",\"demas\",\"despues\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"durante\",\"e\",\"ejemplo\",\"ella\",\"ello\",\"embargo\",\"encuentra\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estan\",\"estaban\",\"estar\",\"estara\",\"estas\",\"este\",\"esto\",\"estos\",\"estuvo\",\"ex\",\"existe\",\"existen\",\"explico\",\"expreso\",\"fuera\",\"gran\",\"grandes\",\"habia\",\"habian\",\"haber\",\"habra\",\"hacerlo\",\"hacia\",\"haciendo\",\"han\",\"hasta\",\"hay\",\"haya\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"hoy\",\"hubo\",\"igual\",\"indico\",\"informo\",\"junto\",\"lado\",\"le\",\"les\",\"llego\",\"lleva\",\"llevar\",\"luego\",\"lugar\",\"mas\",\"manera\",\"manifesto\",\"mayor\",\"me\",\"mediante\",\"mejor\",\"menciono\",\"menos\",\"mi\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"nada\",\"nadie\",\"ni\",\"ningun\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"no\",\"nosotras\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"otra\",\"otras\",\"otros\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podra\",\"podran\",\"podria\",\"podrian\",\"poner\",\"posible\",\"proximo\",\"proximos\",\"primer\",\"primera\",\"primeros\",\"principalmente\",\"propia\",\"propias\",\"propio\",\"propios\",\"pudo\",\"pueda\",\"pues\",\"que\",\"que\",\"quedo\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"realizo\",\"realizado\",\"realizar\",\"respecto\",\"si\",\"solo\",\"se\",\"senalo\",\"sea\",\"sean\",\"segun\",\"segunda\",\"segundo\",\"seis\",\"sera\",\"seran\",\"seria\",\"sido\",\"siempre\",\"siete\",\"sigue\",\"siguiente\",\"sino\",\"sola\",\"solas\",\"solos\",\"son\",\"tal\",\"tampoco\",\"tan\",\"tanto\",\"tenia\",\"tendra\",\"tendran\",\"tenga\",\"tenido\",\"tercera\",\"toda\",\"todas\",\"todavia\",\"todos\",\"total\",\"trata\",\"traves\",\"tres\",\"tuvo\",\"usted\",\"varias\",\"varios\",\"veces\",\"ver\",\"vez\",\"y\",\"ya\"})\n",
        "stopwords_sp = list(stopwords_sp_web_accent_less)\n"
      ],
      "metadata": {
        "id": "cnMcSGcnTe2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect_tf = TfidfVectorizer(\n",
        "    use_idf = False,            # default=True\n",
        "    lowercase = True,           # Lowercase: default=True\n",
        "    strip_accents = 'unicode',  # Remover acentos\n",
        "    stop_words = stopwords_sp,  # Palabras para no tener en cuenta ya que no agregan mucho significado a una oracion.\n",
        "    max_features = 500,         # maximo de vocabulario\n",
        "    max_df = 0.8,               # (frecuencia máxima de documentos) es un valor que representa el porcentaje máximo de documentos en los que un término puede aparecer para ser incluido en el vocabulario. Los términos que aparecen en más del max_df% de los documentos se consideran comunes y, por lo tanto, no son relevantes para el análisis.\n",
        "    min_df = 2,                 # (frecuencia mínima de documentos) es un valor entero que representa la frecuencia mínima con la que un término debe aparecer en los documentos para ser incluido en el vocabulario. Los términos que aparecen en menos de min_df documentos no se consideran relevantes para el análisis y se omiten del vocabulario.\n",
        "    analyzer = 'word',\n",
        "    #norm = None                 # default = 'l2'\n",
        "    #ngram_range=(2,6)\n",
        ")\n",
        "\n",
        "vect_tfidf = TfidfVectorizer(\n",
        "    use_idf = True,             # default=True\n",
        "    lowercase = True,           # Lowercase: default=True\n",
        "    strip_accents = 'unicode',  # Remover acentos\n",
        "    stop_words = stopwords_sp,  # Palabras para no tener en cuenta ya que no agregan mucho significado a una oracion.\n",
        "    max_features = 500,         # maximo de vocabulario\n",
        "    max_df = 0.8,               # (frecuencia máxima de documentos) es un valor que representa el porcentaje máximo de documentos en los que un término puede aparecer para ser incluido en el vocabulario. Los términos que aparecen en más del max_df% de los documentos se consideran comunes y, por lo tanto, no son relevantes para el análisis.\n",
        "    min_df = 2,                 # (frecuencia mínima de documentos) es un valor entero que representa la frecuencia mínima con la que un término debe aparecer en los documentos para ser incluido en el vocabulario. Los términos que aparecen en menos de min_df documentos no se consideran relevantes para el análisis y se omiten del vocabulario.\n",
        "    analyzer = 'word',\n",
        "    #norm = 'l1'                 # default = 'l2'\n",
        "    #ngram_range=(2,6)\n",
        ")\n",
        "\n",
        "vect_binary = CountVectorizer(\n",
        "    binary = True,              # default=True\n",
        "    lowercase = True,           # Lowercase: default=True\n",
        "    strip_accents = 'unicode',  # Remover acentos\n",
        "    stop_words = stopwords_sp,  # Palabras para no tener en cuenta ya que no agregan mucho significado a una oracion.\n",
        "    max_features = 500,         # maximo de vocabulario\n",
        "    max_df = 0.8,               # (frecuencia máxima de documentos) es un valor que representa el porcentaje máximo de documentos en los que un término puede aparecer para ser incluido en el vocabulario. Los términos que aparecen en más del max_df% de los documentos se consideran comunes y, por lo tanto, no son relevantes para el análisis.\n",
        "    min_df = 2,                 # (frecuencia mínima de documentos) es un valor entero que representa la frecuencia mínima con la que un término debe aparecer en los documentos para ser incluido en el vocabulario. Los términos que aparecen en menos de min_df documentos no se consideran relevantes para el análisis y se omiten del vocabulario.\n",
        "    analyzer = 'word',\n",
        "    #ngram_range=(2,6),\n",
        ")\n",
        "\n",
        "vect_count = CountVectorizer(\n",
        "    binary = False,             # default=True\n",
        "    lowercase = True,           # Lowercase: default=True\n",
        "    strip_accents = 'unicode',  # Remover acentos\n",
        "    stop_words = stopwords_sp,  # Palabras para no tener en cuenta ya que no agregan mucho significado a una oracion.\n",
        "    max_features = 500,         # maximo de vocabulario\n",
        "    max_df = 0.8,               # (frecuencia máxima de documentos) es un valor que representa el porcentaje máximo de documentos en los que un término puede aparecer para ser incluido en el vocabulario. Los términos que aparecen en más del max_df% de los documentos se consideran comunes y, por lo tanto, no son relevantes para el análisis.\n",
        "    min_df = 2,                 # (frecuencia mínima de documentos) es un valor entero que representa la frecuencia mínima con la que un término debe aparecer en los documentos para ser incluido en el vocabulario. Los términos que aparecen en menos de min_df documentos no se consideran relevantes para el análisis y se omiten del vocabulario.\n",
        "    analyzer = 'word',\n",
        "    #ngram_range=(2,6),\n",
        ")\n"
      ],
      "metadata": {
        "id": "kz2E0x8BWsng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the Representation\n",
        "\n",
        "# 0: TF           # (float) Frecuencia de términos mide la frecuencia con la que aparece un término en un documento.\n",
        "# 1: TFIDF        # (float) TF por la IDF (Aumenta el peso de los terminos menos frecuentes y disminuye el peso de los terminos mas frecuentes)\n",
        "# 2: BINARY       # (int) Coloca 1 o 0 si la palabra está contenida en el documento.\n",
        "# 3: COUNT        # (int) Coloca el total de palabras que aparecen en un documento.\n",
        "\n",
        "representations = [ vect_tf, vect_tfidf, vect_binary, vect_count ]\n",
        "represent_names = [ 'TF', 'TFIDF', 'BINARY', 'COUNT' ]\n",
        "\n",
        "REP = 3\n",
        "\n",
        "vect    = representations[REP]\n",
        "X_train = vect.fit_transform(texts) # Return: sparse matrix of (n_samples, n_features) Tf-idf-weighted document-term matrix.\n"
      ],
      "metadata": {
        "id": "yqEdu8f2X4ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "0_9Es3U3aDId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifiers\n",
        "\n",
        "#SVM\n",
        "svc = svm.SVC()\n",
        "parameters_svc = [\n",
        "    {'kernel': ['linear'], 'C':[ 10, 100, 1000, 100000], 'gamma': [0.001, 0.01, 0.1, 1 ]},\n",
        "    {'kernel': ['rbf', 'poly', 'sigmoid'], 'gamma':[1e-3, 1e-4,'scale','auto'], 'C':[1, 10, 100, 1000, 100000]}\n",
        "]\n",
        "\n",
        "#LogisticRegression\n",
        "lr = LogisticRegression(solver='liblinear', random_state=2)\n",
        "parameters_lr = [{'solver':['sag', 'newton-cg',  'saga', 'liblinear'], 'max_iter':[1000]}]\n",
        "\n",
        "#KNN\n",
        "knn = Pipeline([('mms', MaxAbsScaler()), ('knn', KNeighborsClassifier())])\n",
        "parameters_knn = [{'knn__n_neighbors': [1, 3, 5, 7, 8, 9, 10], 'knn__weights': ['uniform', 'distance'], 'knn__leaf_size': [15, 20]}]\n",
        "\n",
        "#NaiveBayes Gaussian\n",
        "gnb = GaussianNB()\n",
        "parameters_gnb = [{'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]}]\n",
        "\n",
        "#NaiveBayes Multinomial\n",
        "gnm = MultinomialNB()\n",
        "parameters_gnm = [{'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 'fit_prior' : [True, False] }]\n",
        "\n",
        "#RandomForest\n",
        "rf = RandomForestClassifier() #Initialize with whatever parameters you want to\n",
        "parameters_rf = [{'n_estimators': [5, 10, 15, 20], 'max_depth': [2, 5, 7, 9]}]\n",
        "\n",
        "#DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "parameters_dtc = [{\n",
        "    'splitter': ['best', 'random'], 'criterion': [\"gini\", \"entropy\"], \"max_depth\": [*range(1, 10)],\n",
        "    'min_samples_leaf': [*range(1, 50, 5)],#' min_impurity_decrease': [*np.linspace(0, 0.5, 20)]\n",
        "    #'max_features':[ None, 'auto','sqrt','log2'],   #Added\n",
        "    #'min_impurity_decrease': [None, 0.5]            #Added\n",
        "}]\n",
        "\n",
        "# #DecisionTreeRegressor\n",
        "# dtr = DecisionTreeRegressor()\n",
        "# parameters_dtr =[{\n",
        "#     \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
        "#     \"splitter\":[\"best\",\"random\"], \"max_depth\" : [1,3,5,7,9,11,12], \"max_features\":[\"auto\",\"log2\",\"sqrt\",None], \"max_leaf_nodes\":[None,10,20,30,40,50,60,70,80,90],\n",
        "#     \"min_samples_leaf\":[1,2,3,4,5,6,7,8,9,10], \"min_weight_fraction_leaf\":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "# }]\n",
        "\n",
        "# Group the Classifier with the corresponding params\n",
        "\n",
        "classifier_and_params = [\n",
        "  {'cslf':dtc , 'params':parameters_dtc, 'name':'dtc'},\n",
        "  {'cslf':rf , 'params':parameters_rf, 'name':'rf'},\n",
        "  {'cslf':knn , 'params':parameters_knn, 'name':'knn'},\n",
        "  {'cslf':svc , 'params':parameters_svc, 'name':'svc'},\n",
        "  {'cslf':gnb , 'params':parameters_gnb, 'name':'gnb'},\n",
        "  {'cslf':gnm , 'params':parameters_gnm, 'name':'gnm'},\n",
        "  {'cslf':lr , 'params':parameters_lr, 'name':'lr'}\n",
        "]\n",
        "\n",
        "#### GridSearch ####\n",
        "# Para NB (NaiveBayes) se necesita tener X_traing.toarray(), ya que es este el tipo de valor que espera\n",
        "\n",
        "def gridSearchProcess(classifier, params, name):\n",
        "  clf = GridSearchCV(\n",
        "      estimator=classifier,               # Pass the model instance for which you want to check the hyperparameters.\n",
        "      param_grid=params,                  # the dictionary object that holds the hyperparameters you want to try\n",
        "      scoring='f1_weighted',                 # evaluation metric that you want to use, you can simply pass a valid string/ object of evaluation metric\n",
        "      cv=KFold(n_splits=5, shuffle=True, random_state=1), # number of cross-validation you have to try for each selected set of hyperparameters\n",
        "      #refit=True,                          # Refit an estimator using the best found parameters on the whole dataset (default=True).\n",
        "      #n_jobs=-1,                         # number of processes you wish to run in parallel for this task if it -1 it will use all available processors.\n",
        "      #verbose=1,                         # you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n",
        "  )\n",
        "  clf.fit(X_train.toarray(), labels)\n",
        "\n",
        "  # find best model score\n",
        "  best_params       = clf.best_params_\n",
        "  best_model_score  = clf.score(X_train.toarray(), labels)\n",
        "\n",
        "  print(f'\\n******** {name} Results ********')\n",
        "  #print(f'best_model_score: {round(best_model_score, 4)}')\n",
        "  print(f'best_score:       {round(clf.best_score_, 4)}')\n",
        "  print(f'scorer:           {clf.scoring}')\n",
        "  #print(f'best_estimator:{clf.best_estimator_}')\n",
        "  #print(f'best_index:       {clf.best_index_}')\n",
        "  #print(f'X_traing:         {len(X_train.toarray())}')\n",
        "  #print(f'n_splits:         {clf.n_splits_}')\n",
        "  print(f'best_params:      {clf.best_params_}')\n",
        "\n",
        "print(f'Selected Tokenized Representation: {represent_names[REP]}')\n",
        "print(f'Selected Class Personality: {personality[POS]}')\n",
        "\n",
        "for pair in classifier_and_params:\n",
        "    gridSearchProcess(classifier=pair['cslf'], params=pair['params'], name=pair['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy6DtOeoaE1Q",
        "outputId": "4a2ac9ea-e5b9-4988-a7e9-0b89629c5d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Tokenized Representation: COUNT\n",
            "Selected Class Personality: Responsabilidad\n",
            "\n",
            "******** dtc Results ********\n",
            "best_score:       0.8438\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 31, 'splitter': 'best'}\n",
            "\n",
            "******** rf Results ********\n",
            "best_score:       0.8691\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'max_depth': 9, 'n_estimators': 20}\n",
            "\n",
            "******** knn Results ********\n",
            "best_score:       0.6563\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'knn__leaf_size': 15, 'knn__n_neighbors': 1, 'knn__weights': 'uniform'}\n",
            "\n",
            "******** svc Results ********\n",
            "best_score:       0.8797\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "\n",
            "******** gnb Results ********\n",
            "best_score:       0.8755\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'var_smoothing': 0.001}\n",
            "\n",
            "******** gnm Results ********\n",
            "best_score:       0.8839\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'alpha': 0.1, 'fit_prior': True}\n",
            "\n",
            "******** lr Results ********\n",
            "best_score:       0.8466\n",
            "scorer:           f1_weighted\n",
            "best_params:      {'max_iter': 1000, 'solver': 'saga'}\n"
          ]
        }
      ]
    }
  ]
}